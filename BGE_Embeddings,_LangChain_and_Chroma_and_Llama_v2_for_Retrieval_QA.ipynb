{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahmenyousifMohamed/ML/blob/main/BGE_Embeddings%2C_LangChain_and_Chroma_and_Llama_v2_for_Retrieval_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYwZWWfysS1O"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h2>BGE Embeddings,LangChain and Chroma and Llama v2 for Retrieval QA\n",
        "</h2>\n",
        "</div>\n",
        "\n",
        "Embeddings play a pivotal role in atural language modeling, particularly in the context of semantic search and retrieval augmented generation (RAG).  \n",
        "\n",
        "To see the performance of various embedding models, it is common for practitioners to consult leaderboards.\n",
        "Massive Text Embedding Benchmark (MTEB) Leaderboard from HuggingFace provides well-rounded benchmarks for commonly used embedding models in English and Chinese languages. (also check the Open LLM Leaderboard)  \n",
        "\n",
        "We have been using embeddings from NLP Group of The University of Hong Kong (instructor-xl) for building applications and OpenAI (text-embedding-ada-002) for building quick prototypes.  \n",
        "\n",
        "We recently switched to BGE embeddings (large and base) which are now top-rated on the MTEB leaderboard! What's really impressive is how efficient they are. For example, the bigger version of the BGE model is only 1.34GB, which is much smaller than the 'instructor-xl' model at 4.96GB, but it works even better.  \n",
        "\n",
        "In this tutorial, you will learn how to   \n",
        "- Download papers from Arxiv,   \n",
        "- Create and store embeddings in ChromaDB for RAG,   \n",
        "- Use Llama-2–13B to answer questions and give credit to the sources.  \n",
        "\n",
        "Before we go through all of this awesomeness, please follow us on <a href=\"https://medium.com/@datadrifters\">Medium</a> to never miss a beat.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ZTnHuIsS1R"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "Run the following commands in your terminal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install python3.10-venv"
      ],
      "metadata": {
        "id": "cMn1ZOMJsq0W",
        "outputId": "10cd8211-4811-4a61-bd4c-bd8a0e0afffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl python3.10-venv\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 2,473 kB of archives.\n",
            "After this operation, 2,884 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip-whl all 22.0.2+dfsg-1ubuntu0.4 [1,680 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-setuptools-whl all 59.6.0-1.2ubuntu0.22.04.1 [788 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.3 [5,716 B]\n",
            "Fetched 2,473 kB in 1s (3,261 kB/s)\n",
            "Selecting previously unselected package python3-pip-whl.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pip-whl_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Selecting previously unselected package python3-setuptools-whl.\n",
            "Preparing to unpack .../python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3.10-venv.\n",
            "Preparing to unpack .../python3.10-venv_3.10.12-1~22.04.3_amd64.deb ...\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.3) ...\n",
            "Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "--br7vrFsS1R"
      },
      "outputs": [],
      "source": [
        "# # Create project folder and virtual environment, then install required libraries\n",
        "# !mkdir bge-langchain-chroma && cd bge-langchain-chroma\n",
        "# !python3 -m venv bge-langchain-chroma-env\n",
        "# !source bge-langchain-chroma-env/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_tFuYdwsS1S"
      },
      "source": [
        "install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "0gUuXG2psS1S"
      },
      "outputs": [],
      "source": [
        "!pip3 install langchain tiktoken chromadb python-dotenv ipykernel jupyter arxiv pymupdf\n",
        "!pip3 install sentence_transformers pypdf unstructured\n",
        "!pip3 install auto_gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhlAra_IsS1S"
      },
      "source": [
        "Then open the IDE of your choice, we are using VSCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "yASthgblsS1S"
      },
      "outputs": [],
      "source": [
        "# code ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU5nFcsPsS1S"
      },
      "source": [
        "We are ready to start, let's import required libraries, we have added notes for you to understand what each library does"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OszODnmfsS1T"
      },
      "source": [
        "# Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnFwBo-OsS1T"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from chromadb.config import Settings\n",
        "from urllib.error import HTTPError\n",
        "from dataclasses import replace\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tiktoken # OpenAI's open-source tokenizer\n",
        "import chromadb\n",
        "import logging\n",
        "import random # to sample multiple elements from a list\n",
        "import arxiv\n",
        "import time\n",
        "import os # operating system dependent functionality, to walk through directories and files\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # recursively tries to split by different characters to find one that works\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader # loads pdfs from a given directory\n",
        "from langchain.chains import ConversationalRetrievalChain # looks up relevant documents from the retriever per history and question.\n",
        "from langchain.text_splitter import CharacterTextSplitter # splits the content\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings # wrapper for HuggingFaceBgeEmbeddings models\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import ArxivLoader # loads paper for a given id from Arxiv\n",
        "from langchain.document_loaders import PyPDFLoader # loads a given pdf\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import TextLoader # loads a given text\n",
        "from langchain.retrievers import ArxivRetriever # loads relevant papers for a given paper id from Arxiv\n",
        "from chromadb.utils import embedding_functions # loads Chroma's embedding functions from OpenAI, HuggingFace, SentenceTransformer and others\n",
        "from langchain.chat_models import ChatOpenAI # wrapper around OpenAI LLMs\n",
        "from langchain.vectorstores import Chroma # wrapper around ChromaDB embeddings platform\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain import HuggingFaceHub # wrapper around HuggingFaceHub models\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "load_dotenv() # loads env variables\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO) # to inspect network behavior and API logic of Arxiv and Chroma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkr1kmk9sS1T"
      },
      "source": [
        "load_dotenv() helps you to load environment variables from '.env' file in your root directory. Here's where you typically put your API Keys such as OpenAI, Supabase, Pinecone or other cloud services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLqOpRTzsS1U"
      },
      "source": [
        "## Downloading data: Arxiv paper for a given search term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dni_d9TGsS1U"
      },
      "source": [
        "We will be working with 85-page long Arxiv paper named \"A Survey of Large Language Models\".\n",
        "Here's a little snippet to download it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g7u3GWHSsS1U",
        "outputId": "2aa22400-66b1-4e3c-ef1c-667519390baa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘arxiv_papers’: File exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-42c6c6d1deb7>:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in tqdm(search.results()):\n",
            "1it [00:04,  4.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Paper id 2303.18223v13 with title 'A Survey of Large Language Models' is downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir arxiv_papers\n",
        "dirpath = \"arxiv_papers\"\n",
        "\n",
        "search = arxiv.Search(\n",
        "  query = \"2303.18223\" # ID of the paper A Survey of Large Language Models\n",
        ")\n",
        "\n",
        "for result in tqdm(search.results()):\n",
        "    result.download_pdf(dirpath=dirpath)\n",
        "    print(f\"-> Paper id {result.get_short_id()} with title '{result.title}' is downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYmXGQmJsS1V"
      },
      "source": [
        "We created a directory called \"arxiv_papers\" in the current working directory and download the paper there.\n",
        "You can now load all the papers in that directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mg5ocWW3sS1V"
      },
      "outputs": [],
      "source": [
        "papers = []\n",
        "loader = DirectoryLoader('./arxiv_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "papers = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pj7MOluqsS1V",
        "outputId": "feb19c83-f8d9-4dbf-e392-e2a92bc66d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of pages loaded:  124\n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of pages loaded: \", len(papers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyYJUrfXsS1W"
      },
      "source": [
        "Before we split the text into smaller chunks, let us explain two important arguments: chunk_size and chunk_overlap\n",
        "\n",
        "When you're trying to embed a document, you have to think about the granularity of the information that you are trying to capture. Sometimes you need a fine-grained view (e.g., spell-checks, keyword analysis), and other times, you need to take a step back to see the greater context (e.g., summarization, question-answering).\n",
        "\n",
        "So, depending on what you're trying to understand from a text, you'll need to adjust how much you read at one time, which is controlled by chunk_size, and chunk_overlap is the number of characters to overlap between two chunks for preserving the semantic context in subsequent chunks.\n",
        "\n",
        "In addition, chunk_size also has an effect on the inference performance since it determines the average number of tokens that will be submitted to LLM to generate the response.\n",
        "\n",
        "There are different chunking strategies and <a href=\"https://www.pinecone.io/learn/chunking-strategies/\">here's a nice article</a> that explains several options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU3JKklnsS1W"
      },
      "source": [
        "Let's load and chunk the paper using chunk size of 500 and overlap of 50, you should definitely experiment with these to find what works best in your case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nBc_0acisS1W"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 50\n",
        ")\n",
        "\n",
        "paper_chunks = text_splitter.split_documents(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NLBP8vUwsS1W",
        "outputId": "b8744b91-b6cc-43d8-dd8c-7b32e98a7a04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1668"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(paper_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l20Z6aQEsS1W"
      },
      "source": [
        "You can manually inspect some of the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eioNnrcMsS1W",
        "outputId": "984550be-a52b-4c0b-c4d2-99d3acbab493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI\\ncommunity, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this\\nsurvey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular,', metadata={'source': 'arxiv_papers/2303.18223v13.A_Survey_of_Large_Language_Models.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "paper_chunks[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkQRKaNZsS1W"
      },
      "source": [
        "You can also verify the average length of the chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VDnhOe1ksS1W",
        "outputId": "10b91430-d0e1-46a5-9b6e-207c6a23a469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "452.8890887290168"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "chunk_lengths = [len(paper_chunk.page_content) for paper_chunk in paper_chunks]\n",
        "np.average(chunk_lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2SgORu-sS1X"
      },
      "source": [
        "Looks good! Let's continue with embeddings.\n",
        "\n",
        "If you are not familiar with the topic, you can think of using embeddings as creating a smart filing system where files are semantically related. Similar files will be closer to each other than dissimilar ones. It helps you to quickly find and use relevant information as per user prompt or query.\n",
        "\n",
        "Technically, embeddings enables the dynamic augmentation of the model input at execution time, in addition to your prompt, you also provide relevant context for model to generate high quality responses.\n",
        "\n",
        "Let's see how it's done in practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyzZHZEasS1X"
      },
      "source": [
        "# Downloading HuggingFace BGE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KH-Eidt-sS1X"
      },
      "outputs": [],
      "source": [
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "embedding_function = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziUXjjU0sS1X"
      },
      "source": [
        "# Working with ChromaDB to store embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKH29jbfsS1X"
      },
      "source": [
        "In this section, you will use the OpenAI embedding model to generate embeddings for your documents and store them in ChromaDB for easy retrieval later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kVp5f8jUsS1X"
      },
      "outputs": [],
      "source": [
        "persist_directory=\"./chromadb/\"\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=paper_chunks, # text data that you want to embed and store\n",
        "    embedding=embedding_function, # used to convert the documents into embeddings\n",
        "    persist_directory=persist_directory, # this tells Chroma where to store its data\n",
        "    collection_name=\"arxiv_papers\" #  gives a name to the collection of embeddings, which will be helpful for retrieving specific groups of embeddings later.\n",
        ")\n",
        "\n",
        "vectordb.persist() # will make the database save any changes to the disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvibU5ZIsS1X"
      },
      "source": [
        "# Retrieval QA with LangChain and Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoj7F3wDsS1X"
      },
      "source": [
        "In case you run this code block second time after ChromaDB is created, you can use below line to create vectordb from ChromaDB. This will save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hgLAgwcUsS1X"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install auto-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk2BRmBf7vj_",
        "outputId": "2d4e869b-52cd-41d9-9915-fb0ff5f39815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gsYMbWQlJk-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r \"/content/TheBloke/Me\""
      ],
      "metadata": {
        "id": "pkayukhFJeJD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-IrvprIsS1Y"
      },
      "source": [
        "First, we need to download Llama-2-13B-chat-GPTQ model, but you can also use 7B or 30B models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FEmfh7NRsS1Y",
        "outputId": "acbb106d-7b95-407f-bb3d-15f4d7917119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - `checkpoint_format` is missing from the quantization configuration and is automatically inferred to gptq.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "WARNING:auto_gptq.utils.accelerate_utils:Some weights of the model checkpoint at /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GPTQ/snapshots/ea078917a7e91c896787c73dba935f032ae658e9/model.safetensors were not used when initializing LlamaForCausalLM: {'model.layers.37.self_attn.o_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.33.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.down_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.38.mlp.down_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.18.mlp.down_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.19.mlp.down_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.5.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.up_proj.bias', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.32.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.16.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
          ]
        }
      ],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "model_basename = \"model\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "use_safetensors=True,\n",
        "trust_remote_code=True,\n",
        "device=\"cuda:0\",\n",
        "use_triton=use_triton,\n",
        "quantize_config=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
        "# !pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIkJJ2E1_XSv",
        "outputId": "ed0c1482-2fa7-46b7-946f-01a0ca3d4498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 uninstall -y auto-gptq\n",
        "# !git clone https://github.com/PanQiWei/AutoGPTQ\n",
        "# !cd AutoGPTQ\n",
        "# !pip3 install .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpktutBb_Xt3",
        "outputId": "0ef7d7f9-f73c-4477-c730-aee7db4326a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: auto_gptq 0.7.1\n",
            "Uninstalling auto_gptq-0.7.1:\n",
            "  Successfully uninstalled auto_gptq-0.7.1\n",
            "Cloning into 'AutoGPTQ'...\n",
            "remote: Enumerating objects: 4872, done.\u001b[K\n",
            "remote: Counting objects: 100% (1471/1471), done.\u001b[K\n",
            "remote: Compressing objects: 100% (284/284), done.\u001b[K\n",
            "remote: Total 4872 (delta 1335), reused 1193 (delta 1187), pack-reused 3401\u001b[K\n",
            "Receiving objects: 100% (4872/4872), 8.11 MiB | 12.39 MiB/s, done.\n",
            "Resolving deltas: 100% (3225/3225), done.\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd AutoGPTQ\n",
        "# !ls\n",
        "# !pip3 install ."
      ],
      "metadata": {
        "id": "Hrb9nD7O_oNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install optimum"
      ],
      "metadata": {
        "id": "3CPrbZSeCu8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install auto-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHpSNkKMCxxu",
        "outputId": "b2e8e54e-dbd2-410f-d0db-6513e57fa898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.8.0.dev0+cu1222)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.29.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.19.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.25.2)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.1.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.2.1+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.38.2)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajObSj2HsS1Y"
      },
      "source": [
        "creating the HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hlxWV4vcsS1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12d1cb0-38ba-42f4-efa4-92190c0e0b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15)\n",
        "print()\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss2M4pZesS1Y"
      },
      "source": [
        "creating the QA chain with retriever to answer the questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8_j7ZceusS1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fa461e2-9a5b-4b74-c843-3386fc64be1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Load chain from chain type.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "RetrievalQA.from_chain_type.__doc__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NHCKhhvMsS1c"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uVibBG7sS1c"
      },
      "source": [
        "We are ready to ask questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QqGb2oJHsS1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041000c3-2ec0-4bba-a06b-fd48e03b522c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the recent advancements in LLMs?\"\n",
        "llm_response = retrieval_qa_chain(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ygDaGCRosS1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16ecb61-7a9f-4a89-b497-8c469ef7d5e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\",\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Question: What are the recent advancements in LLMs?',\n",
              " 'Helpful Answer: Recent advancements in LLMs include the development of transformer-based models, such as BERT and RoBERTa, which have achieved state-of-the-art results on a wide range of NLP tasks. These models use self-supervised learning techniques to learn high-level semantic representations of language, which can be fine-tuned for specific downstream tasks like sentiment analysis or machine translation. Additionally, there has been growing interest in multimodal LLMs that can process and integrate information from multiple sources, such as text, images, and audio. Finally, there is also research on Explainable AI (XAI) techniques to understand how LLMs make decisions and generate text.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "llm_response['result'].split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPGmMhHhsS1d"
      },
      "source": [
        "We can also see the source and pages to generate the answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vMk52OExsS1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895e4402-327b-4d04-ae2f-63805e96b169"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "[source.metadata for source in llm_response[\"source_documents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFMGr8B8sS1f"
      },
      "source": [
        "You can also use retrieval QA chain with prompt templates, here's how you would do it for the same example as above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LBtCd-wIsS1f"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "{summaries}\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "retrieval_qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"summaries\", \"question\"],\n",
        "        ),\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZS3P-GOYsS1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682e4a15-69dc-40a3-e4bb-0f7e6fcf1808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the recent advancements in LLMs?\"\n",
        "llm_response = retrieval_qa_chain(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TKaqd29WsS1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ea0b12-b0d4-4b76-ccd9-b5a421b56867"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What are the recent advancements in LLMs?',\n",
              " 'answer': '\\n\\nWhat are the recent advancements in LLMs?\\n---------------------------------------\\n\\nLLMs have been rapidly evolving over the past few years, with several recent advancements that have improved their performance and applicability. Some of these advancements include:\\n\\n1. **Attention Mechanisms**: Attention mechanisms were introduced to improve the ability of LLMs to focus on specific parts of the input data, allowing them to better capture long-range dependencies and handle input sequences of varying lengths.\\n2. **Pre-trained Language Models**: Pre-trained language models like BERT, RoBERTa, and XLNet have achieved state-of-the-art results on a wide range of NLP tasks, including question answering, sentiment analysis, named entity recognition, and text classification. These models use a multi-layer bidirectional transformer encoder to learn high-level semantic and syntactic features of language.\\n3. **Transformers**: Transformers are a type of neural network architecture that have revolutionized the field of NLP. They are particularly well-suited for sequence-to-sequence tasks, such as machine translation, text summarization, and speech recognition. Transformers have also been used to improve the performance of LLMs.\\n4. **Multitask Learning**: Multitask learning is a technique that involves training a single model on multiple tasks simultaneously. This can help improve the generalization abilities of LLMs by exposing them to a wider range of linguistic phenomena.\\n5. **Adversarial Training**: Adversarial training involves training an LLM to be robust against attacks designed to manipulate its predictions. This can help improve the reliability and security of LLMs in real-world applications.\\n6. **Explainable AI**: Explainable AI is a growing area of research that aims to provide insights into how LLMs make decisions. Techniques such as feature importance, saliency maps, and attention visualization can help users understand why an LLM has made a particular prediction or decision.\\n7. **Human-AI Collaboration**: Human-AI collaboration involves using LLMs to augment human capabilities, rather than replacing them. For example, LLMs can be used to assist humans in generating creative content, such as stories or poems, or in helping humans to better understand complex data sets.\\n8. **Ethical Considerations**',\n",
              " 'sources': '',\n",
              " 'source_documents': []}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "llm_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpvtMTB_sS1g"
      },
      "source": [
        "Nice, hope this helps! We'll be around if you have any questions.\n",
        "\n",
        "See you somewhere in the matrix!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgM3LQZrsS1g"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}